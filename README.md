# Big Data Project: Social Media Engagement & Productivity Analysis

## ğŸ“Œ Project Overview
This project analyzes video engagement patterns on social media to predict user satisfaction, identify addiction levels, and understand productivity loss. The dataset is stored in **MongoDB Atlas**, processed using **Dask**, analyzed with **Pandas**, visualized using **Matplotlib & Seaborn**, and deployed as a **Streamlit dashboard**.

## ğŸ“‚ Folder Structure
big_data_project/ 
    â”‚â”€â”€ data/ # Store raw and processed data â”‚ 
        â”œâ”€â”€ raw_data.csv # Original dataset (if applicable) â”‚ 
            â”œâ”€â”€ processed_data.parquet/ # Processed Parquet files â”‚ 
        â”œâ”€â”€ visualizations/ # Saved visualization images 
    â”‚â”€â”€ notebooks/ # Jupyter notebooks for EDA and visualization â”‚ 
        â”œâ”€â”€ eda.ipynb # Exploratory Data Analysis (Step 3) â”‚ 
        â”œâ”€â”€ analysis.ipynb # Advanced analysis (Step 4) â”‚ 
        â”œâ”€â”€ visualization.ipynb # Data visualization (Step 5) 
    â”‚â”€â”€ scripts/ # Python scripts for automation â”‚ 
        â”œâ”€â”€ step1_upload_mongo.py # Upload raw data to MongoDB Atlas â”‚ 
        â”œâ”€â”€ step2_process_data.py # Process data using Dask â”‚ 
        â”œâ”€â”€ step4_analysis.py # Advanced analytics using Pandas & Dask 
    â”‚â”€â”€ cloud/ # Cloud deployment setup â”‚ 
        â”œâ”€â”€ streamlit_dashboard.py # Streamlit dashboard for visualization 
    â”‚â”€â”€ requirements.txt # Python dependencies 
    â”‚â”€â”€ config.py # Configuration settings (MongoDB URI, etc.) 
    â”‚â”€â”€ README.md # Project documentation & instructions   (this file)

## ğŸš€ Installation & Setup

### **Step 1: Install Dependencies**
Run the following command to install required Python libraries:
```sh
pip install -r requirements.txt
```

## ***Step 2: Set Up MongoDB Atlas**
Sign up on MongoDB Atlas.
Create a free cluster.
Create a database bigdata_project and collection social_media_analysis.
Copy your MongoDB connection string and store it in config.py.

## ***Step 3: Upload Data to MongoDB**
Run the script to upload the dataset:
```sh
python scripts/step1_upload_mongo.py
```

## ***Step 4: Process Data using Dask**
```sh
python scripts/step2_process_data.py
```

## ***Step 5: Perform Exploratory Data Analysis (EDA)**
Launch Jupyter Notebook:
```sh
jupyter notebook
```
Open the eda.ipynb notebook and execute it.

## ***Step 6: Run Advanced Analytics with Pandas & Dask**
```sh
python scripts/step4_analysis.py
```

## ***Step 7: Generate Visualizations**
Open and run notebooks/visualization.ipynb in Jupyter Notebook.

## ***Step 8: Deploy Dashboard**
Run the Streamlit dashboard:
```sh
streamlit run cloud/streamlit_dashboard.py
```

ğŸ“Š Features
âœ”ï¸ MongoDB Atlas for NoSQL storage
âœ”ï¸ Dask for large-scale data processing
âœ”ï¸ Pandas for data analysis
âœ”ï¸ Seaborn & Matplotlib for visualization
âœ”ï¸ Streamlit for cloud-based interactive dashboard

ğŸ“Œ Dataset
The dataset used in this project is from Kaggle: "Time-Wasters on Social Media".
It contains user behavior, engagement, and addiction-level data across multiple platforms.

ğŸ’¡ Future Improvements
Implement real-time data ingestion using Kafka (optional)
Deploy on AWS/GCP for scalability
Use Machine Learning for predictive analytics